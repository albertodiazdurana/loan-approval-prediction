{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f5b5aa",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction - Binary Classification\n",
    "\n",
    "**Notebook:** w01_d01_EDA_baseline_models.ipynb  \n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Build baseline classification models to predict loan approval status for interview preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Perform exploratory data analysis on loan application dataset (~600 records)\n",
    "- Preprocess data: handle missing values, encode categorical features, engineer income ratios\n",
    "- Train and evaluate 3 baseline models: Logistic Regression, Decision Tree, Random Forest\n",
    "- Compare model performance using accuracy, precision, recall, and F1-score\n",
    "\n",
    "## Business Context\n",
    "\n",
    "This analysis prepares a working baseline for a 50-minute technical interview, demonstrating end-to-end data science workflow from data quality assessment through model evaluation and selection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102818af",
   "metadata": {},
   "source": [
    "### Section 1: Setup & Environment Configuration\n",
    "\n",
    "Import required Python libraries for data analysis, visualization, and machine learning. Configure display settings, define project directory paths, and verify data file accessibility before beginning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073116e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup & Environment Configuration\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "OUTPUTS = PROJECT_ROOT / 'outputs' / 'figures' / 'eda'\n",
    "\n",
    "# Verify correct paths\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_RAW}\")\n",
    "print(f\"Output directory: {OUTPUTS}\")\n",
    "print(f\"Data file exists: {(DATA_RAW / 'loans_modified.csv').exists()}\")\n",
    "print(f\"Dict file exists: {(DATA_RAW / 'data_dictionary.txt').exists()}\")\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09926947",
   "metadata": {},
   "source": [
    "### Section 2: Data Loading & Validation\n",
    "\n",
    "Load the loan applications dataset and perform initial validation checks. Verify data shape, examine first rows, review data types, and generate summary statistics to understand the dataset structure before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Loading & Validation\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_RAW / 'loans_modified.csv')\n",
    "\n",
    "# Basic information\n",
    "print(\"Dataset Shape:\")\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Data types and non-null counts\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17b39a",
   "metadata": {},
   "source": [
    "### Section 3: Missing Values Analysis\n",
    "\n",
    "Identify and quantify missing values across all features. Visualize missing data patterns to inform preprocessing strategy and understand data quality issues that need to be addressed before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180062f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Missing Values Analysis\n",
    "\n",
    "# Calculate missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Create summary dataframe\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_percentages\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal features with missing values: {(missing_counts > 0).sum()} -> out of {df.shape[1]}\")\n",
    "print(f\"Features with >5% missing: {(missing_percentages > 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d7d97",
   "metadata": {},
   "source": [
    "**Missing Values Identified**\n",
    "Key findings:\n",
    "\n",
    "- All 13 features have missing values\n",
    "- 6 features >5% missing (self_employed, coapplicant_income, dependents, loan_amount, loan_id, gender)\n",
    "- Critical: Target variable loan_status has 28 missing values (4.97%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22562a",
   "metadata": {},
   "source": [
    "### Section 4: Target Variable Distribution\n",
    "\n",
    "Analyze the distribution of the target variable (loan_status) to understand class balance between approved and rejected loans. This informs whether class imbalance techniques will be needed during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Target Variable Distribution\n",
    "\n",
    "# Remove missing values from target for analysis\n",
    "df_target = df['loan_status'].dropna()\n",
    "\n",
    "# Calculate distribution\n",
    "target_counts = df_target.value_counts()\n",
    "target_percentages = df_target.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(f\"\\nApproved (1): {target_counts[1.0]:.0f} ({target_percentages[1.0]:.2f}%)\")\n",
    "print(f\"Rejected (0): {target_counts[0.0]:.0f} ({target_percentages[0.0]:.2f}%)\")\n",
    "print(f\"\\nClass Ratio (Approved:Rejected): {target_percentages[1.0]/target_percentages[0.0]:.2f}:1\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['salmon', 'lightgreen'])\n",
    "axes[0].set_title('Loan Status Distribution (Counts)')\n",
    "axes[0].set_xlabel('Loan Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Rejected (0)', 'Approved (1)'], rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts, labels=['Rejected (0)', 'Approved (1)'], \n",
    "            autopct='%1.1f%%', startangle=90, colors=['salmon', 'lightgreen'])\n",
    "axes[1].set_title('Loan Status Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass imbalance assessment: {'Moderate imbalance detected' if target_percentages[1.0] > 65 or target_percentages[1.0] < 35 else 'Relatively balanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7432fa4",
   "metadata": {},
   "source": [
    "Key Findings:\n",
    "May need class_weight parameter or SMOTE during modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1405cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Feature Distributions\n",
    "\n",
    "# Key numeric features\n",
    "numeric_features = ['applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term']\n",
    "categorical_features = ['gender', 'married', 'dependents', 'education', 'self_employed', \n",
    "                       'credit_history', 'property_area']\n",
    "\n",
    "print(\"Numeric Feature Distributions:\")\n",
    "print(df[numeric_features].describe())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Visualize numeric features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numeric_features):\n",
    "    df[feature].dropna().hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feature} Distribution')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'numeric_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d871e8",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "\n",
    "- Income skew: Applicant income max (81K) vs median (3.8K) indicates outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCategorical Feature Distributions:\")\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(df[feature].value_counts())\n",
    "    \n",
    "# Calculate key percentages for insights\n",
    "print(\"\\nKey Feature Insights:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Credit history percentage\n",
    "credit_positive = (df['credit_history'] == 1.0).sum()\n",
    "credit_total = df['credit_history'].notna().sum()\n",
    "credit_pct = (credit_positive / credit_total) * 100\n",
    "print(f\"Credit History: {credit_positive}/{credit_total} ({credit_pct:.1f}%) have positive credit history\")\n",
    "\n",
    "# Gender distribution\n",
    "gender_male = (df['gender'] == 'Male').sum()\n",
    "gender_total = df['gender'].notna().sum()\n",
    "gender_pct = (gender_male / gender_total) * 100\n",
    "print(f\"Gender: {gender_male}/{gender_total} ({gender_pct:.1f}%) are Male\")\n",
    "\n",
    "# Self-employed\n",
    "self_emp_no = (df['self_employed'] == 'No').sum()\n",
    "self_emp_total = df['self_employed'].notna().sum()\n",
    "self_emp_pct = (self_emp_no / self_emp_total) * 100\n",
    "print(f\"Self-Employed: {self_emp_no}/{self_emp_total} ({self_emp_pct:.1f}%) are not self-employed\")\n",
    "\n",
    "# Education\n",
    "edu_grad = (df['education'] == 'Graduate').sum()\n",
    "edu_total = df['education'].notna().sum()\n",
    "edu_pct = (edu_grad / edu_total) * 100\n",
    "print(f\"Education: {edu_grad}/{edu_total} ({edu_pct:.1f}%) are Graduates\")\n",
    "\n",
    "# Married\n",
    "married_yes = (df['married'] == 'Yes').sum()\n",
    "married_total = df['married'].notna().sum()\n",
    "married_pct = (married_yes / married_total) * 100\n",
    "print(f\"Married: {married_yes}/{married_total} ({married_pct:.1f}%) are Married\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ec2d5",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "\n",
    "- Credit history: Strong predictor - 88% have positive credit history\n",
    "- Gender: Male-dominated dataset (81%)\n",
    "- Loan term: Most loans are 360 months (30 years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27a8c4",
   "metadata": {},
   "source": [
    "### Section 6: Data Preprocessing\n",
    "\n",
    "Handle missing values using appropriate imputation strategies, remove rows with missing target variable, and prepare dataset for feature engineering. This ensures clean data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove rows with missing target variable (cannot train on these)\n",
    "print(f\"Original dataset size: {len(df_clean)}\")\n",
    "df_clean = df_clean.dropna(subset=['loan_status'])\n",
    "print(f\"After removing missing target: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Handle missing values - Numeric features (use median)\n",
    "numeric_features = ['applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term', 'credit_history']\n",
    "\n",
    "print(\"\\nImputing numeric features with median:\")\n",
    "for feature in numeric_features:\n",
    "    if df_clean[feature].isnull().sum() > 0:\n",
    "        median_value = df_clean[feature].median()\n",
    "        df_clean[feature].fillna(median_value, inplace=True)\n",
    "        print(f\"  {feature}: filled {df[feature].isnull().sum()} missing values with {median_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Handle missing values - Categorical features (use mode)\n",
    "categorical_features = ['gender', 'married', 'dependents', 'education', 'self_employed', 'property_area']\n",
    "\n",
    "print(\"\\nImputing categorical features with mode:\")\n",
    "for feature in categorical_features:\n",
    "    if df_clean[feature].isnull().sum() > 0:\n",
    "        mode_value = df_clean[feature].mode()[0]\n",
    "        df_clean[feature].fillna(mode_value, inplace=True)\n",
    "        print(f\"  {feature}: filled {df[feature].isnull().sum()} missing values with '{mode_value}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(df_clean.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nFinal dataset shape:\")\n",
    "print(f\"Rows: {df_clean.shape[0]}\")\n",
    "print(f\"Columns: {df_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fabe22",
   "metadata": {},
   "source": [
    "Issue: 27 missing values remain after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c90454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns still have missing values\n",
    "print(\"Remaining missing values by column:\")\n",
    "print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68ce66",
   "metadata": {},
   "source": [
    "The loan_id is just an identifier, not a feature. We'll drop it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop loan_id (not needed for modeling - just an identifier)\n",
    "print(f\"Dropping loan_id column (identifier, not a feature)\")\n",
    "df_clean = df_clean.drop('loan_id', axis=1)\n",
    "\n",
    "print(f\"\\nDataset shape after dropping loan_id: {df_clean.shape}\")\n",
    "print(f\"Remaining missing values: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify clean dataset\n",
    "print(\"\\nFinal verification:\")\n",
    "print(df_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31f174",
   "metadata": {},
   "source": [
    "Clean dataset ready:\n",
    "\n",
    "- 535 rows (removed 28 with missing target)\n",
    "- 12 features (dropped loan_id identifier)\n",
    "- 0 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b35c0",
   "metadata": {},
   "source": [
    "### Section 7: Feature Engineering\n",
    "\n",
    "Create new features to improve model performance: total household income (applicant + coapplicant) and income-to-loan ratio to capture affordability. These engineered features provide additional context for loan approval prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "# Create total_income feature\n",
    "df_clean['total_income'] = df_clean['applicant_income'] + df_clean['coapplicant_income']\n",
    "\n",
    "# Create income_to_loan_ratio (handle division by zero)\n",
    "df_clean['income_to_loan_ratio'] = df_clean['total_income'] / (df_clean['loan_amount'] + 0.001)  # +0.001 to avoid division by zero\n",
    "\n",
    "print(\"Engineered Features Created:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. total_income = applicant_income + coapplicant_income\")\n",
    "print(f\"   Mean: {df_clean['total_income'].mean():.2f}\")\n",
    "print(f\"   Median: {df_clean['total_income'].median():.2f}\")\n",
    "print(f\"   Range: [{df_clean['total_income'].min():.0f}, {df_clean['total_income'].max():.0f}]\")\n",
    "\n",
    "print(\"\\n2. income_to_loan_ratio = total_income / loan_amount\")\n",
    "print(f\"   Mean: {df_clean['income_to_loan_ratio'].mean():.2f}\")\n",
    "print(f\"   Median: {df_clean['income_to_loan_ratio'].median():.2f}\")\n",
    "print(f\"   Range: [{df_clean['income_to_loan_ratio'].min():.2f}, {df_clean['income_to_loan_ratio'].max():.2f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nTotal features after engineering: {df_clean.shape[1]}\")\n",
    "print(f\"Feature list: {list(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb619875",
   "metadata": {},
   "source": [
    "14 features total (12 original + 2 engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b874927",
   "metadata": {},
   "source": [
    "### Section 8: Categorical Encoding\n",
    "\n",
    "Encode categorical variables into numeric format required for machine learning models. Use label encoding for binary features and one-hot encoding for multi-class features to preserve information while making data model-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94261b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Categorical Encoding\n",
    "\n",
    "# Create a copy for encoding\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Binary features - map to 0/1\n",
    "binary_mappings = {\n",
    "    'gender': {'Male': 1, 'Female': 0},\n",
    "    'married': {'Yes': 1, 'No': 0},\n",
    "    'education': {'Graduate': 1, 'Not Graduate': 0},\n",
    "    'self_employed': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "print(\"Encoding Binary Features:\")\n",
    "print(\"=\"*60)\n",
    "for feature, mapping in binary_mappings.items():\n",
    "    df_encoded[feature] = df_encoded[feature].map(mapping)\n",
    "    print(f\"{feature}: {mapping}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Ordinal feature - dependents\n",
    "dependents_mapping = {'0': 0, '1': 1, '2': 2, '3+': 3}\n",
    "df_encoded['dependents'] = df_encoded['dependents'].map(dependents_mapping)\n",
    "print(f\"\\nEncoding Ordinal Feature:\")\n",
    "print(f\"dependents: {dependents_mapping}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# One-hot encoding for property_area (nominal)\n",
    "print(f\"\\nOne-Hot Encoding property_area:\")\n",
    "property_dummies = pd.get_dummies(df_encoded['property_area'], prefix='property', drop_first=True)\n",
    "df_encoded = pd.concat([df_encoded, property_dummies], axis=1)\n",
    "df_encoded = df_encoded.drop('property_area', axis=1)\n",
    "print(f\"Created columns: {list(property_dummies.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nFinal encoded dataset shape: {df_encoded.shape}\")\n",
    "print(f\"All features numeric: {df_encoded.select_dtypes(include=['object']).shape[1] == 0}\")\n",
    "\n",
    "print(\"\\nFirst 3 rows of encoded data:\")\n",
    "print(df_encoded.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d0b82",
   "metadata": {},
   "source": [
    "15 features ready for modeling (all numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print df_encoded shape\n",
    "print(f\"\\nEncoded dataset shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f712f75",
   "metadata": {},
   "source": [
    "### Section 9: Train/Test Split\n",
    "\n",
    "Separate features from target variable and split data into training (80%) and testing (20%) sets. This enables model training on one subset and unbiased evaluation on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da770f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Train/Test Split\n",
    "\n",
    "# Convert boolean columns to integers for consistency\n",
    "df_encoded['property_Semiurban'] = df_encoded['property_Semiurban'].astype(int)\n",
    "df_encoded['property_Urban'] = df_encoded['property_Urban'].astype(int)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "print(\"Features and Target Separated:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "print(list(X.columns))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Split into train and test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test Split (80/20):\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(f\"  Approved (1): {(y_train == 1.0).sum()} ({(y_train == 1.0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Rejected (0): {(y_train == 0.0).sum()} ({(y_train == 0.0).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget distribution in testing set:\")\n",
    "print(f\"  Approved (1): {(y_test == 1.0).sum()} ({(y_test == 1.0).sum()/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Rejected (0): {(y_test == 0.0).sum()} ({(y_test == 0.0).sum()/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e6248",
   "metadata": {},
   "source": [
    "428 training samples, 107 test samples\n",
    "Class distribution maintained in both sets (~72% approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65601c5",
   "metadata": {},
   "source": [
    "### Section 10: Baseline Model Training\n",
    "\n",
    "Train three baseline classification models (Logistic Regression, Decision Tree, Random Forest) using default hyperparameters. These models establish performance benchmarks for comparison and identify the most promising approach for further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Baseline Model Training\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train models and store predictions\n",
    "results = {}\n",
    "\n",
    "print(\"Training Baseline Models:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  {name} trained successfully\")\n",
    "    print(f\"    Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"    Precision: {precision:.4f}\")\n",
    "    print(f\"    Recall:    {recall:.4f}\")\n",
    "    print(f\"    F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abb588",
   "metadata": {},
   "source": [
    "Performance Summary:\n",
    "\n",
    "- Random Forest: Best accuracy (80.37%) and F1 (87.12%)\n",
    "- Logistic Regression: Close second (79.44% accuracy, 87.06% F1)\n",
    "- Decision Tree: Lowest performance (72.90% accuracy, 81.29% F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5f81e",
   "metadata": {},
   "source": [
    "### Section 11: Model Comparison & Evaluation\n",
    "\n",
    "Compare model performance across all metrics and visualize confusion matrices. Identify the best-performing model and analyze error patterns to understand strengths and weaknesses of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Model Comparison & Evaluation\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
    "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
    "    'F1-Score': [results[m]['f1_score'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, comparison_df[metric], width, label=metric, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac806706",
   "metadata": {},
   "source": [
    "### Section 12: Confusion Matrices\n",
    "\n",
    "Visualize confusion matrices for all three models to understand prediction patterns and error types. This reveals how many loans were correctly/incorrectly classified as approved or rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Confusion Matrices\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                cbar=False, square=True)\n",
    "    axes[idx].set_title(f'{name}\\nF1-Score: {result[\"f1_score\"]:.4f}')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xticklabels(['Rejected (0)', 'Approved (1)'])\n",
    "    axes[idx].set_yticklabels(['Rejected (0)', 'Approved (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  True Negatives (Correctly Rejected):  {tn}\")\n",
    "    print(f\"  False Positives (Incorrectly Approved): {fp}\")\n",
    "    print(f\"  False Negatives (Incorrectly Rejected): {fn}\")\n",
    "    print(f\"  True Positives (Correctly Approved):   {tp}\")\n",
    "    print(f\"  Total Correct: {tn + tp}/{len(y_test)} ({(tn + tp)/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a5b59",
   "metadata": {},
   "source": [
    "Error Analysis:\n",
    "\n",
    "- Logistic Regression: High recall (96%) but many false positives (19)\n",
    "- Random Forest: Best balance - fewer false negatives (6) and good accuracy\n",
    "- Decision Tree: Most false negatives (14) - rejects too many valid loans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec8ca1",
   "metadata": {},
   "source": [
    "PROJECT SUMMARY\n",
    "============================================================\n",
    "\n",
    "Dataset:\n",
    "  Total samples: 563 (535 after cleaning)\n",
    "  Features: 14 (12 original + 2 engineered)\n",
    "  Target distribution: 72% approved, 28% rejected\n",
    "\n",
    "Key Findings:\n",
    "  1. Credit history is likely the strongest predictor (88% positive)\n",
    "  2. Moderate class imbalance (2.54:1 ratio)\n",
    "  3. Income features show high variability (outliers present)\n",
    "  4. Engineered features (total_income, income_to_loan_ratio) added context\n",
    "\n",
    "Model Performance:\n",
    "  Best Model: Random Forest\n",
    "    - Accuracy: 80.4%\n",
    "    - F1-Score: 87.1%\n",
    "    - Best balance of precision (82.6%) and recall (92.2%)\n",
    "\n",
    "============================================================\n",
    "\n",
    "NEXT STEPS: Advanced Techniques\n",
    "============================================================\n",
    "1. Hyperparameter Tuning:\n",
    "   - GridSearchCV on Random Forest (n_estimators, max_depth, min_samples_split)\n",
    "   - Try different train/test splits or cross-validation\n",
    "\n",
    "2. Address Class Imbalance:\n",
    "   - Test SMOTE (Synthetic Minority Over-sampling)\n",
    "   - Adjust class_weight parameter in models\n",
    "\n",
    "3. Feature Engineering:\n",
    "   - Test polynomial features or interactions\n",
    "   - Feature selection using feature importance or RFE\n",
    "\n",
    "4. Advanced Models:\n",
    "   - Gradient Boosting (XGBoost, LightGBM)\n",
    "   - Ensemble methods (Voting Classifier, Stacking)\n",
    "\n",
    "5. Model Interpretability:\n",
    "   - Feature importance visualization\n",
    "   - SHAP values for explaining predictions\n",
    "\n",
    "6. Evaluation:\n",
    "   - ROC curve and AUC score\n",
    "   - Cross-validation for robust performance estimates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f65a1",
   "metadata": {},
   "source": [
    "### Section 14: Hyperparameter Tuning - Random Forest\n",
    "\n",
    "Optimize Random Forest hyperparameters using GridSearchCV with cross-validation. Test different combinations of n_estimators, max_depth, and min_samples_split to improve model performance beyond baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b89be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. Hyperparameter Tuning - Random Forest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Hyperparameter Tuning with GridSearchCV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(f\"\\nTesting {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])} parameter combinations\")\n",
    "print(\"\\nParameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running GridSearchCV (this may take a minute)...\\n\")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "rf_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in rf_tuned.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation F1-Score: {rf_tuned.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_tuned = rf_tuned.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy_tuned:.4f} (baseline: {results['Random Forest']['accuracy']:.4f})\")\n",
    "print(f\"  Precision: {precision_tuned:.4f} (baseline: {results['Random Forest']['precision']:.4f})\")\n",
    "print(f\"  Recall:    {recall_tuned:.4f} (baseline: {results['Random Forest']['recall']:.4f})\")\n",
    "print(f\"  F1-Score:  {f1_tuned:.4f} (baseline: {results['Random Forest']['f1_score']:.4f})\")\n",
    "\n",
    "improvement = f1_tuned - results['Random Forest']['f1_score']\n",
    "print(f\"\\nF1-Score Improvement: {improvement:+.4f} ({improvement/results['Random Forest']['f1_score']*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28c092",
   "metadata": {},
   "source": [
    "Finding: Optimal parameters already close to baseline (n_estimators=50, max_depth=10)\n",
    "No improvement needed - baseline was already well-tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb3132",
   "metadata": {},
   "source": [
    "### Section 15: Address Class Imbalance with SMOTE\n",
    "\n",
    "Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the training data by generating synthetic samples of the minority class. Compare performance with and without SMOTE to assess impact on model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3250956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15. Address Class Imbalance with SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Handling Class Imbalance with SMOTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original class distribution\n",
    "print(\"\\nOriginal Training Set Distribution:\")\n",
    "print(f\"  Approved (1): {(y_train == 1.0).sum()} ({(y_train == 1.0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Rejected (0): {(y_train == 0.0).sum()} ({(y_train == 0.0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Ratio: {(y_train == 1.0).sum()/(y_train == 0.0).sum():.2f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(f\"  Approved (1): {(y_train_smote == 1.0).sum()} ({(y_train_smote == 1.0).sum()/len(y_train_smote)*100:.1f}%)\")\n",
    "print(f\"  Rejected (0): {(y_train_smote == 0.0).sum()} ({(y_train_smote == 0.0).sum()/len(y_train_smote)*100:.1f}%)\")\n",
    "print(f\"  Ratio: {(y_train_smote == 1.0).sum()/(y_train_smote == 0.0).sum():.2f}:1\")\n",
    "print(f\"  Total samples: {len(X_train)} -> {len(X_train_smote)} (+{len(X_train_smote) - len(X_train)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Random Forest with SMOTE data...\\n\")\n",
    "\n",
    "# Train Random Forest with SMOTE\n",
    "rf_smote = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    max_depth=10, \n",
    "    random_state=42\n",
    ")\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_smote = rf_smote.predict(X_test)\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "precision_smote = precision_score(y_test, y_pred_smote)\n",
    "recall_smote = recall_score(y_test, y_pred_smote)\n",
    "f1_smote = f1_score(y_test, y_pred_smote)\n",
    "\n",
    "print(\"Performance with SMOTE:\")\n",
    "print(f\"  Accuracy:  {accuracy_smote:.4f} (baseline: {results['Random Forest']['accuracy']:.4f})\")\n",
    "print(f\"  Precision: {precision_smote:.4f} (baseline: {results['Random Forest']['precision']:.4f})\")\n",
    "print(f\"  Recall:    {recall_smote:.4f} (baseline: {results['Random Forest']['recall']:.4f})\")\n",
    "print(f\"  F1-Score:  {f1_smote:.4f} (baseline: {results['Random Forest']['f1_score']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Confusion matrix comparison\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "tn_smote, fp_smote, fn_smote, tp_smote = cm_smote.ravel()\n",
    "\n",
    "print(\"\\nConfusion Matrix Comparison:\")\n",
    "print(f\"                    Baseline  |  SMOTE\")\n",
    "print(f\"True Negatives:     {15:8}  |  {tn_smote:8}\")\n",
    "print(f\"False Positives:    {15:8}  |  {fp_smote:8}\")\n",
    "print(f\"False Negatives:    {6:8}  |  {fn_smote:8}\")\n",
    "print(f\"True Positives:     {71:8}  |  {tp_smote:8}\")\n",
    "\n",
    "print(f\"\\nSMOTE Impact: {'Improved minority class detection' if tn_smote > 15 else 'Similar to baseline'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76af3d",
   "metadata": {},
   "source": [
    "Finding: SMOTE decreased performance\n",
    "\n",
    "- Accuracy dropped: 80.4% → 75.7% (-4.7%)\n",
    "- F1-score dropped: 0.871 → 0.842 (-3.0%)\n",
    "- Conclusion: Original class distribution is optimal; SMOTE not beneficial for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb8893",
   "metadata": {},
   "source": [
    "### Section 16: Feature Importance Analysis\n",
    "\n",
    "Analyze which features contribute most to Random Forest predictions. Understanding feature importance helps identify key drivers of loan approval decisions and validates domain knowledge about credit history and income factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 16. Feature Importance Analysis\n",
    "\n",
    "print(\"Feature Importance Analysis - Random Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importances from best model\n",
    "best_rf_model = rf_tuned.best_estimator_\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "ax.barh(importance_df_sorted['Feature'], importance_df_sorted['Importance'], color='steelblue')\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_title('Feature Importance - Random Forest')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "top_3_features = importance_df.head(3)['Feature'].tolist()\n",
    "print(f\"  Top 3 features: {', '.join(top_3_features)}\")\n",
    "print(f\"  Credit history importance: {importance_df[importance_df['Feature'] == 'credit_history']['Importance'].values[0]:.4f}\")\n",
    "print(f\"  Combined income features: {importance_df[importance_df['Feature'].isin(['applicant_income', 'coapplicant_income', 'total_income'])]['Importance'].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6c69f",
   "metadata": {},
   "source": [
    "Key Findings:\n",
    "\n",
    "- Credit history dominates (24.3%) - validates domain knowledge\n",
    "- Engineered feature success: income_to_loan_ratio is 2nd most important (14.4%)\n",
    "- Income features critical: Combined 32.8% importance\n",
    "- Demographics less important: married, gender, education < 2% each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95122d",
   "metadata": {},
   "source": [
    "### Section 17: ROC Curve and AUC Score\n",
    "\n",
    "Evaluate model performance using ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) scores. These metrics assess the model's ability to distinguish between approved and rejected loans across different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 17. ROC Curve and AUC Score\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print(\"ROC Curve and AUC Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predicted probabilities for all models\n",
    "models_roc = {\n",
    "    'Logistic Regression': results['Logistic Regression']['model'],\n",
    "    'Decision Tree': results['Decision Tree']['model'],\n",
    "    'Random Forest (Tuned)': rf_tuned.best_estimator_\n",
    "}\n",
    "\n",
    "# Calculate ROC curves and AUC scores\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "print(\"\\nAUC Scores:\")\n",
    "for idx, (name, model) in enumerate(models_roc.items()):\n",
    "    # Get predicted probabilities\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', \n",
    "            color=colors[idx], linewidth=2)\n",
    "    \n",
    "    print(f\"  {name}: {auc_score:.4f}\")\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5000)', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves - Model Comparison')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / 'roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUC Interpretation:\")\n",
    "print(\"  0.90-1.00: Excellent\")\n",
    "print(\"  0.80-0.90: Good\")\n",
    "print(\"  0.70-0.80: Fair\")\n",
    "print(\"  0.60-0.70: Poor\")\n",
    "print(\"  0.50-0.60: Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19b4c0",
   "metadata": {},
   "source": [
    "Key Findings:\n",
    "\n",
    "- Random Forest: 0.8065 (Good) - Strong discriminative ability\n",
    "- Decision Tree: 0.6591 (Poor) - Weak discrimination\n",
    "- Logistic Regression: 0.5866 (Poor) - Barely better than random!\n",
    "\n",
    "Insight: Random Forest significantly outperforms others in probability ranking, even though accuracy scores were similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d6bff",
   "metadata": {},
   "source": [
    "### Section 18: Cross-Validation for Robust Evaluation\n",
    "\n",
    "Perform 5-fold cross-validation to obtain more reliable performance estimates. This reduces variance from single train/test split and provides confidence intervals for model performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db109403",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 18. Cross-Validation for Robust Evaluation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "print(\"Cross-Validation Analysis (5-Fold)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Models to evaluate\n",
    "cv_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "# Scoring metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "cv_results_summary = []\n",
    "\n",
    "for name, model in cv_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                               scoring=scoring, return_train_score=False)\n",
    "    \n",
    "    # Calculate means and stds\n",
    "    results_dict = {\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{cv_scores['test_accuracy'].mean():.4f} ± {cv_scores['test_accuracy'].std():.4f}\",\n",
    "        'Precision': f\"{cv_scores['test_precision'].mean():.4f} ± {cv_scores['test_precision'].std():.4f}\",\n",
    "        'Recall': f\"{cv_scores['test_recall'].mean():.4f} ± {cv_scores['test_recall'].std():.4f}\",\n",
    "        'F1-Score': f\"{cv_scores['test_f1'].mean():.4f} ± {cv_scores['test_f1'].std():.4f}\"\n",
    "    }\n",
    "    \n",
    "    cv_results_summary.append(results_dict)\n",
    "    \n",
    "    print(f\"  Accuracy:  {cv_scores['test_accuracy'].mean():.4f} ± {cv_scores['test_accuracy'].std():.4f}\")\n",
    "    print(f\"  Precision: {cv_scores['test_precision'].mean():.4f} ± {cv_scores['test_precision'].std():.4f}\")\n",
    "    print(f\"  Recall:    {cv_scores['test_recall'].mean():.4f} ± {cv_scores['test_recall'].std():.4f}\")\n",
    "    print(f\"  F1-Score:  {cv_scores['test_f1'].mean():.4f} ± {cv_scores['test_f1'].std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "cv_summary_df = pd.DataFrame(cv_results_summary)\n",
    "print(cv_summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f180c3",
   "metadata": {},
   "source": [
    "Key Insights:\n",
    "- Cross-validation confirms Random Forest superiority\n",
    "- Low std deviation indicates stable performance across folds\n",
    "- Results align with single train/test split evaluation\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "- Random Forest: Best performer (F1: 0.8854 ± 0.0109)\n",
    "- Most stable: Lowest standard deviations across all metrics\n",
    "- CV confirms test results: 82.0% accuracy (vs 80.4% on single split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18024bca",
   "metadata": {},
   "source": [
    "### Section 19: Final Summary & Recommendations\n",
    "\n",
    "Consolidate all findings from baseline models, hyperparameter tuning, class imbalance handling, feature importance, and cross-validation. Provide actionable recommendations and key takeaways for stakeholder communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b35071",
   "metadata": {},
   "source": [
    "COMPREHENSIVE PROJECT SUMMARY\n",
    "======================================================================\n",
    "\n",
    "1. DATASET OVERVIEW\n",
    "----------------------------------------------------------------------\n",
    "  Original samples: 563\n",
    "  After cleaning: 535 (removed 28 with missing target)\n",
    "  Features: 14 (12 original + 2 engineered)\n",
    "  Class distribution: 72% approved / 28% rejected (2.54:1 ratio)\n",
    "\n",
    "2. PREPROCESSING & FEATURE ENGINEERING\n",
    "----------------------------------------------------------------------\n",
    "  - Imputed missing values: median (numeric), mode (categorical)\n",
    "  - Encoded categorical variables: label encoding + one-hot\n",
    "  - Engineered features:\n",
    "    * total_income (applicant + coapplicant)\n",
    "    * income_to_loan_ratio (total_income / loan_amount)\n",
    "  - Train/test split: 80/20 (428 train, 107 test)\n",
    "\n",
    "3. MODEL PERFORMANCE COMPARISON\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "  Test Set Results:\n",
    "  Model                  Accuracy  Precision  Recall   F1-Score  AUC\n",
    "  Logistic Regression     79.4%     79.6%    96.1%    87.1%    0.587\n",
    "  Decision Tree           72.9%     80.8%    81.8%    81.3%    0.659\n",
    "  Random Forest (Base)    80.4%     82.6%    92.2%    87.1%    0.807\n",
    "  Random Forest (Tuned)   80.4%     82.6%    92.2%    87.1%    0.807\n",
    "\n",
    "  Cross-Validation Results (5-Fold):\n",
    "  Random Forest           82.0%     81.7%    96.7%    88.5%    --\n",
    "\n",
    "4. FEATURE IMPORTANCE (Top 5)\n",
    "----------------------------------------------------------------------\n",
    "  1. credit_history         24.3%  (Domain validated)\n",
    "  2. income_to_loan_ratio   14.4%  (Engineered feature success)\n",
    "  3. applicant_income       12.5%\n",
    "  4. loan_amount            12.3%\n",
    "  5. total_income           11.4%  (Engineered feature success)\n",
    "\n",
    "  Combined income features: 32.8% importance\n",
    "\n",
    "5. OPTIMIZATION ATTEMPTS\n",
    "----------------------------------------------------------------------\n",
    "  Hyperparameter Tuning:\n",
    "    - GridSearchCV tested 108 combinations\n",
    "    - Best params: n_estimators=50, max_depth=10\n",
    "    - Result: No improvement (already optimal)\n",
    "\n",
    "  Class Imbalance (SMOTE):\n",
    "    - Balanced training data (1:1 ratio)\n",
    "    - Result: Performance decreased (-4.7% accuracy)\n",
    "    - Conclusion: Original distribution optimal\n",
    "\n",
    "6. KEY INSIGHTS\n",
    "----------------------------------------------------------------------\n",
    "  - Credit history is dominant predictor (validates domain knowledge)\n",
    "  - Engineered features highly valuable (income_to_loan_ratio #2)\n",
    "  - Random Forest significantly outperforms other models on AUC\n",
    "  - Model robust across folds (low std: ±0.011 F1-score)\n",
    "  - Class imbalance not problematic for this dataset\n",
    "\n",
    "7. FINAL RECOMMENDATION\n",
    "----------------------------------------------------------------------\n",
    "  MODEL: Random Forest (n_estimators=50, max_depth=10)\n",
    "  EXPECTED PERFORMANCE:\n",
    "    - Accuracy: ~82% (cross-validated)\n",
    "    - F1-Score: 0.885 ± 0.011\n",
    "    - AUC: 0.807 (good discrimination)\n",
    "\n",
    "  STRENGTHS:\n",
    "    - Best overall performance across all metrics\n",
    "    - Stable predictions (low variance)\n",
    "    - High recall (96.7%) - catches most approved loans\n",
    "    - Interpretable via feature importance\n",
    "\n",
    "  BUSINESS VALUE:\n",
    "    - Reduces manual review workload by 82%\n",
    "    - Minimizes false rejections (only 6 missed approvals)\n",
    "    - Explainable decisions via feature importance\n",
    "\n",
    "8. FUTURE ENHANCEMENTS (If More Time)\n",
    "----------------------------------------------------------------------\n",
    "  - Test gradient boosting (XGBoost, LightGBM)\n",
    "  - Explore feature interactions (polynomial features)\n",
    "  - SHAP values for individual prediction explanations\n",
    "  - Threshold tuning for business-specific cost functions\n",
    "  - Ensemble methods (stacking, voting)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
